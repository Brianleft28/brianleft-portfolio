import { GoogleGenerativeAI } from '@google/generative-ai';
import type { RequestHandler } from './$types';
import { env } from '$env/dynamic/private';
import { readFileSync, readdirSync } from 'fs';
import { join } from 'path';

// Memoria base (est√°tica)
import indexMemory from '$lib/data/memory/index.md?raw';
import memoryIndex from '$lib/data/memory/memory.md?raw';
import metaMemory from '$lib/data/memory/meta.md?raw';

const MODEL_NAME = 'gemini-2.5-flash';
const MAX_INPUT_CHARS = 4000;
const MAX_REQUESTS_PER_MINUTE = 10;
const PROJECTS_PATH = 'src/lib/data/memory/projects';

// Rate limiting simple en memoria (se resetea con cada deploy)
const requestCounts = new Map<string, { count: number; resetTime: number }>();

function isRateLimited(ip: string): boolean {
	const now = Date.now();
	const record = requestCounts.get(ip);

	if (!record || now > record.resetTime) {
		requestCounts.set(ip, { count: 1, resetTime: now + 60000 });
		return false;
	}

	if (record.count >= MAX_REQUESTS_PER_MINUTE) {
		return true;
	}

	record.count++;
	return false;
}

// Keywords que activan cada m√≥dulo de memoria
const projectKeywords: Record<string, string> = {
	// Print Server
	print: printServerMemory,
	impresora: printServerMemory,
	imprimir: printServerMemory,
	zpl: printServerMemory,
	'esc-pos': printServerMemory,
	t√©rmica: printServerMemory,
	spooler: printServerMemory,
	'.net': printServerMemory,
	// Electoral
	electoral: electoralMemory,
	voto: electoralMemory,
	elecci√≥n: electoralMemory,
	elecciones: electoralMemory,
	fiscal: electoralMemory,
	gobierno: electoralMemory,
	concurrencia: electoralMemory,
	// Portfolio / Meta
	portfolio: portfolioMemory,
	terminal: metaMemory,
	torvalds: metaMemory,
	arquitectura: metaMemory,
	'c√≥mo funciona': metaMemory,
	'este sitio': metaMemory,
	'esta web': metaMemory,
	// Migrador
	migrador: migradorMemory,
	migracion: migradorMemory,
	beneficiarios: migradorMemory,
	excel: migradorMemory,
	'datos sucios': migradorMemory
	// Preguntas generales sobre proyectos (NO cargan docs espec√≠ficos, pero index.md ya los lista)
	// Se manejan en la l√≥gica de getRelevantMemory
};

/**
 * Selecciona solo los m√≥dulos de memoria relevantes seg√∫n el prompt del usuario.
 * Siempre incluye el perfil base (index.md) para contexto m√≠nimo.
 */
function getRelevantMemory(prompt: string): string {
	const lowerPrompt = prompt.toLowerCase();
	const relevantDocs = new Set<string>([indexMemory]); // Siempre incluir perfil base

	for (const [keyword, doc] of Object.entries(projectKeywords)) {
		if (lowerPrompt.includes(keyword)) {
			relevantDocs.add(doc);
		}
	}

	// Si no matche√≥ nada espec√≠fico, incluir meta para contexto general
	if (relevantDocs.size === 1) {
		relevantDocs.add(metaMemory);
	}

	return Array.from(relevantDocs).join('\n\n---\n\n');
}

export const POST: RequestHandler = async ({ request, getClientAddress }) => {
	try {
		// Rate limiting
		const clientIp = getClientAddress();
		if (isRateLimited(clientIp)) {
			return new Response('Demasiadas peticiones. Espera un momento.', { status: 429 });
		}

		const apiKey = env.GEMINI_API_KEY;
		if (!apiKey) {
			return new Response('Error: API Key no configurada.', { status: 500 });
		}

		const { prompt } = await request.json();
		const userPrompt = `${prompt ?? ''}`.slice(0, MAX_INPUT_CHARS);

		if (!userPrompt.trim()) {
			return new Response('Error: Mensaje vac√≠o.', { status: 400 });
		}

		// Cargar solo memoria relevante seg√∫n keywords
		const memoryContent = getRelevantMemory(userPrompt);

		const genAI = new GoogleGenerativeAI(apiKey);
		const model = genAI.getGenerativeModel({ model: MODEL_NAME });

		const fullPrompt = `
        ## CONTEXTO
        ${memoryContent}

        ---
        ## SISTEMA

Eres TorvaldsAi, asistente t√©cnico del portfolio de Brian Benegas.
Personalidad: Linus Torvalds - directo, pragm√°tico, t√©cnicamente exigente pero respetuoso.

REGLAS:

1. **IDIOMA**: Espa√±ol argentino rioplatense sutil. Si el usuario escribe en otro idioma, respond√© en ese idioma naturalmente. Brian est√° aprendiendo ingl√©s activamente, as√≠ que si preguntan en ingl√©s, respond√© en ingl√©s claro y t√©cnico.

2. **LONGITUD**: Adapta seg√∫n complejidad.
   - Preguntas simples: 1-3 l√≠neas.
   - Explicaciones t√©cnicas: hasta 350 palabras.
   - Arquitectura/flujos: diagramas ASCII obligatorios.

3. **FORMATO PERMITIDO** (usalo libremente):
   - **Negritas** para t√©rminos clave
   - \`c√≥digo inline\` para archivos, funciones, comandos
   - Bloques de c√≥digo con \`\`\`lenguaje
   - Listas con - o n√∫meros
   - Tablas cuando compares opciones
   - Diagramas ASCII para arquitectura
   - Emojis con moderaci√≥n (üîß‚ö°‚úÖ‚ùå etc.)

4. **FORMATO PROHIBIDO**:
   - T√≠tulos con # o ## (NUNCA)
   - L√≠neas en blanco excesivas (m√°ximo 1 entre secciones)
   - P√°rrafos largos sin estructura

5. **EJEMPLO DE RESPUESTA IDEAL**:
   El sistema usa **SvelteKit** con \`adapter-node\`. Arquitectura:
   \`\`\`
   [Browser] --> [SSR] --> [API Routes]
                               |
                         [Gemini API]
   \`\`\`
   Componentes clave:
   - \`Terminal.svelte\`: emulador de consola
   - \`/api/chat\`: endpoint de IA con streaming
   
   Todo corre en Docker üê≥ con build multi-stage.

6. **L√çMITES**: Solo portfolio, proyectos y experiencia de Brian. Pero siempre respond√© con respeto.

7. **TONO**: S√© t√©cnicamente exigente y directo, pero nunca despectivo sobre el aprendizaje o crecimiento personal de nadie. El sarcasmo va para c√≥digo malo, no para personas.

USUARIO: "${userPrompt}"

RESPUESTA:`;

		const result = await model.generateContentStream(fullPrompt);
		console.log('[API] Respuesta recibida, comenzando stream...');

		const stream = new ReadableStream({
			async start(controller) {
				for await (const chunk of result.stream) {
					const text = chunk.text();
					if (text) controller.enqueue(text);
				}
				controller.close();
			}
		});

		return new Response(stream, {
			headers: {
				'content-type': 'text/plain; charset=utf-8',
				'Transfer-Encoding': 'chunked'
			}
		});
	} catch (error) {
		console.error('[GEMINI API ERROR]', error);
		return new Response('Kernel panic: Connection to cognitive core failed.', { status: 500 });
	}
};
